{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julianmina00/piper-voice/blob/main/notebooks/piper_multilingual_training_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK3nmYDB6C1a"
      },
      "source": [
        "# <font color=\"ffc800\"> **[Piper](https://github.com/rhasspy/piper) training notebook.**\n",
        "## ![Piper logo](https://contribute.rhasspy.org/img/logo.png)\n",
        "\n",
        "---\n",
        "\n",
        "- Notebook made by [rmcpantoja](http://github.com/rmcpantoja)\n",
        "- Collaborator: [Xx_Nessu_xX](http://github.com/Xx_Nessu_xX)\n",
        "\n",
        "---\n",
        "\n",
        "# Notes:\n",
        "\n",
        "- <font color=\"orange\">**Things in orange mean that they are important.**\n",
        "\n",
        "# Credits:\n",
        "\n",
        "* [Feanix-Fyre fork](https://github.com/Feanix-Fyre/piper) with some improvements.\n",
        "* [Tacotron2 NVIDIA training notebook](https://github.com/justinjohn0306/FakeYou-Tacotron2-Notebook) - Dataset duration snippet.\n",
        "* [üê∏TTS](https://github.com/coqui-ai/TTS) - Resampler and XTTS formater demo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AICh6p5OJybj"
      },
      "source": [
        "# <font color=\"ffc800\">üîß ***First steps.*** üîß"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cellView": "form",
        "id": "qyxSMuzjfQrz",
        "outputId": "7b366625-903f-410f-9fd0-11d805912c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown ## <font color=\"ffc800\"> **Google Colab Anti-Disconnect.** üîå\n",
        "#@markdown ---\n",
        "#@markdown #### Avoid automatic disconnection. Still, it will disconnect after <font color=\"orange\">**6 to 12 hours**</font>.\n",
        "\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "display(IPython.display.Javascript(js_code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellView": "form",
        "id": "ygxzp-xHTC7T",
        "outputId": "5f2627ac-8126-48bf-852e-e702c4f66287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 25 23:23:39 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## <font color=\"ffc800\"> **Check GPU type.** üëÅÔ∏è\n",
        "#@markdown ---\n",
        "#@markdown #### A higher capable GPU can lead to faster training speeds. By default, you will have a <font color=\"orange\">**Tesla T4**</font>.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "cellView": "form",
        "id": "sUNjId07JfAK",
        "collapsed": true,
        "outputId": "b558d73c-b561-4208-8051-cb63686d5acf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-74f1ff579cb1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#@markdown ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **Mount Google Drive.** üìÇ\n",
        "#@markdown ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_XwmTVlcUgCh",
        "cellView": "form",
        "outputId": "ea6c51a0-4592-4711-d484-2a41160871d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/piper/src/python\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **Install software.** üì¶\n",
        "#@markdown ---\n",
        "#@markdown ####In this cell the synthesizer and its necessary dependencies to execute the training will be installed. (this may take a while)\n",
        "\n",
        "# clone:\n",
        "!git clone -q https://github.com/rmcpantoja/piper\n",
        "%cd /content/piper/src/python\n",
        "!wget -q \"https://raw.githubusercontent.com/coqui-ai/TTS/dev/TTS/bin/resample.py\"\n",
        "#!pip install -q -r requirements.txt\n",
        "!pip install -q cython>=0.29.0 piper-phonemize==1.1.0 librosa>=0.9.2 numpy==1.26 onnxruntime>=1.15.0 pytorch-lightning\n",
        "!pip install --upgrade gdown transformers\n",
        "!bash build_monotonic_align.sh\n",
        "# Useful vars:\n",
        "use_whisper = True\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3bMzEE0V5Ma"
      },
      "source": [
        "# <font color=\"ffc800\"> ü§ñ ***Training.*** ü§ñ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cellView": "form",
        "id": "SvEGjf0aV8eg",
        "outputId": "ba40291d-34be-4d80-82dd-510fb7d2c198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/dataset\n",
            "Unzipping audio content...\n",
            "replace /content/dataset/wavs/1.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "/content/dataset/wavs\n",
            "Opened dataset with 5 wavs with duration 0:04:10.\n",
            "/content/dataset\n"
          ]
        }
      ],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **1. Extract dataset.** üì•\n",
        "#@markdown ---\n",
        "#@markdown ####Important: the audios must be in <font color=\"orange\">**wav format, (16000 or 22050hz, 16-bits, mono), and, for convenience, numbered. Example:**\n",
        "\n",
        "#@markdown * <font color=\"orange\">**1.wav**</font>\n",
        "#@markdown * <font color=\"orange\">**2.wav**</font>\n",
        "#@markdown * <font color=\"orange\">**3.wav**</font>\n",
        "#@markdown * <font color=\"orange\">**.....**</font>\n",
        "\n",
        "#@markdown ---\n",
        "import os\n",
        "import wave\n",
        "import zipfile\n",
        "import datetime\n",
        "\n",
        "def get_dataset_duration(wav_path):\n",
        "    totalduration = 0\n",
        "    for file_name in [x for x in os.listdir(wav_path) if os.path.isfile(x) and \".wav\" in x]:\n",
        "        with wave.open(file_name, \"rb\") as wave_file:\n",
        "            frames = wave_file.getnframes()\n",
        "            rate = wave_file.getframerate()\n",
        "            duration = frames / float(rate)\n",
        "            totalduration += duration\n",
        "    wav_count = len(os.listdir(wav_path))\n",
        "    duration_str = str(datetime.timedelta(seconds=round(totalduration, 0)))\n",
        "    return wav_count, duration_str\n",
        "\n",
        "%cd /content\n",
        "if not os.path.exists(\"/content/dataset\"):\n",
        "    os.makedirs(\"/content/dataset\")\n",
        "    os.makedirs(\"/content/dataset/wavs\")\n",
        "%cd /content/dataset\n",
        "#@markdown ### Audio dataset path to unzip:\n",
        "zip_path = \"/content/wavs.zip\" #@param {type:\"string\"}\n",
        "zip_path = zip_path.strip()\n",
        "if zip_path:\n",
        "    if os.path.exists(zip_path):\n",
        "        if zipfile.is_zipfile(zip_path):\n",
        "            print(\"Unzipping audio content...\")\n",
        "            !unzip -q -j \"{zip_path}\" -d /content/dataset/wavs\n",
        "        else:\n",
        "            print(\"Copying audio contents of this folder...\")\n",
        "            fp = zip_path + \"/.\"\n",
        "            !cp -a \"$fp\" \"/content/dataset/wavs\"\n",
        "    else:\n",
        "        raise Exception(\"The path provided to the wavs is not correct. Please set a valid path.\")\n",
        "else:\n",
        "    raise Exception(\"You must provide with a path to the wavs.\")\n",
        "if os.path.exists(\"/content/dataset/wavs/wavs\"):\n",
        "    for file in os.listdir(\"/content/dataset/wavs/wavs\"):\n",
        "        !mv /content/dataset/wavs/wavs/\"$file\"  /content/dataset/wavs/\"$file\"\n",
        "    !rm -r /content/dataset/wavs/*.txt\n",
        "    !rm -r /content/dataset/wavs/*.csv\n",
        "%cd /content/dataset/wavs\n",
        "audio_count, dataset_dur = get_dataset_duration(\"/content/dataset/wavs\")\n",
        "print(f\"Opened dataset with {audio_count} wavs with duration {dataset_dur}.\")\n",
        "%cd ..\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cellView": "form",
        "id": "E0W0OCvXXvue",
        "outputId": "122e3ff8-1c04-4a1e-ddaa-d0f0fc938400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-84ca61e0-a367-407a-af26-56a6e1b804ec\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-84ca61e0-a367-407a-af26-56a6e1b804ec\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving matadata.csv to matadata.csv\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **2. Upload the transcript file.** üìù\n",
        "#@markdown ---\n",
        "#@markdown ####<font color=\"orange\">**Important: the transcription means writing what the character says in each of the audios, and it must have the following structure:**\n",
        "\n",
        "#@markdown ##### <font color=\"orange\">For a single-speaker dataset:\n",
        "#@markdown * wavs/1.wav|This is what my character says in audio 1.\n",
        "#@markdown * wavs/2.wav|This, the text that the character says in audio 2.\n",
        "#@markdown * ...\n",
        "\n",
        "#@markdown ##### <font color=\"orange\">For a multi-speaker dataset:\n",
        "\n",
        "#@markdown * wavs/speaker1audio1.wav|speaker1|This is what the first speaker says.\n",
        "#@markdown * wavs/speaker1audio2.wav|speaker1|This is another audio of the first speaker.\n",
        "#@markdown * wavs/speaker2audio1.wav|speaker2|This is what the second speaker says in the first audio.\n",
        "#@markdown * wavs/speaker2audio2.wav|speaker2|This is another audio of the second speaker.\n",
        "#@markdown * ...\n",
        "\n",
        "#@markdown And so on. In addition, the transcript must be in a <font color=\"orange\">**.csv or .txt format. (UTF-8 without BOM)**\n",
        "\n",
        "#@markdown ## Auto-transcribe with whisper if transcription is not provided.\n",
        "\n",
        "#@markdown **Note: If you don't upload any transcription files, the wavs will be transcribed using the whisper tool when you execute the next step. Then, the notebook will continue with the rest of the preprocessing if there are no errors. Although the Whisper tool has good transcription results, in my experience I recommend transcribing manually and uploading it from this cell, since a good TTS voice needs to be optimized to give even better results. For example, when transcribing manually you will be able to observe every detail that the speaker makes (such as punctuation, sounds, etc.), and capture them in the transcription according to the speaker's intonations.**\n",
        "\n",
        "\n",
        "#@markdown However, if you want to transcribe and review this transcription, you can use the individual notebooks:\n",
        "\n",
        "#@markdown * [English](http://colab.research.google.com/github/rmcpantoja/My-Colab-Notebooks/blob/main/notebooks/OpenAI_Whisper_-_DotCSV_(Speech_dataset_multi-transcryption_support)en.ipynb)\n",
        "#@markdown * [French](http://colab.research.google.com/github/rmcpantoja/My-Colab-Notebooks/blob/main/notebooks/OpenAI_Whisper_-_DotCSV_(Speech_dataset_multi-transcryption_support)fr.ipynb)\n",
        "#@markdown * [Spanish](http://colab.research.google.com/github/rmcpantoja/My-Colab-Notebooks/blob/main/notebooks/OpenAI_Whisper_-_DotCSV_(Speech_dataset_multi-transcryption_support)es.ipynb)\n",
        "\n",
        "#@markdown ---\n",
        "%cd /content/dataset\n",
        "from google.colab import files\n",
        "!rm /content/dataset/metadata.csv\n",
        "\n",
        "if os.path.exists(\"/content/dataset/wavs/_transcription.txt\"):\n",
        "  !mv \"/content/dataset/wavs/_transcription.txt\" metadata.csv\n",
        "else:\n",
        "  listfn, length = files.upload().popitem()\n",
        "  if listfn != \"metadata.csv\":\n",
        "    !mv \"$listfn\" metadata.csv\n",
        "\n",
        "use_whisper = False\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dOyx9Y6JYvRF",
        "cellView": "form",
        "outputId": "4faa9299-14cb-41e5-e1c4-f3fd40f7edc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/piper/src/python\n",
            "INFO:preprocess:Single speaker dataset\n",
            "INFO:preprocess:Wrote dataset config\n",
            "INFO:preprocess:Processing 5 utterance(s) with 2 worker(s)\n",
            "Preprocessing done!\n"
          ]
        }
      ],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **3. Preprocess dataset.** üîÑ\n",
        "#@markdown ---\n",
        "import os\n",
        "if use_whisper:\n",
        "    import torch\n",
        "    from faster_whisper import WhisperModel\n",
        "    from tqdm import tqdm\n",
        "    from google import colab\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    def make_dataset(path, language):\n",
        "        metadata = \"\"\n",
        "        text = \"\"\n",
        "        files = [f for f in os.listdir(path) if f.endswith(\".wav\")]\n",
        "        assert len(files) > 0, \"You don't have wavs uploaded either! Please upload at least one zip with the wavs in step 2.\"\n",
        "        metadata_file = open(f\"{path}/../metadata.csv\", \"w\")\n",
        "        whisper = WhisperModel(\"large-v3\", device=device, compute_type=\"float16\")\n",
        "        for audio_file in tqdm(files):\n",
        "            full_path = os.path.join(path, audio_file)\n",
        "            segments, _ = whisper.transcribe(full_path, word_timestamps=False, language=language)\n",
        "            for segment in segments:\n",
        "                text += segment.text\n",
        "            text = text.strip()\n",
        "            text = text.replace('\\n', ' ')\n",
        "            metadata = f\"{audio_file}|{text}\\n\"\n",
        "            metadata_file.write(metadata)\n",
        "            text = \"\"\n",
        "        colab.files.download(f\"{path}/../metadata.csv\")\n",
        "        del whisper\n",
        "        return True\n",
        "\n",
        "#@markdown ### First of all, select the language of your dataset.\n",
        "language = \"Espa√±ol (Castellano)\" #@param [\"ÿ£ŸÑÿπŸéÿ±Ÿéÿ®ŸêŸä\", \"Catal√†\", \"ƒçe≈°tina\", \"Dansk\", \"Deutsch\", \"ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨\", \"English (British)\", \"English (U.S.)\", \"Espa√±ol (Castellano)\", \"Espa√±ol (Latinoamericano)\", \"Suomi\", \"Fran√ßais\", \"Magyar\", \"Icelandic\", \"Italiano\", \"·É•·Éê·É†·Éó·É£·Éö·Éò\", \"“õ–∞–∑–∞“õ—à–∞\", \"L√´tzebuergesch\", \"‡§®‡•á‡§™‡§æ‡§≤‡•Ä\", \"Nederlands\", \"Norsk\", \"Polski\", \"Portugu√™s (Brasil)\", \"Portugu√™s (Portugal)\", \"Rom√¢nƒÉ\", \"–†—É—Å—Å–∫–∏–π\", \"–°—Ä–ø—Å–∫–∏\", \"Svenska\", \"Kiswahili\", \"T√ºrk√ße\", \"—É–∫—Ä–∞—óÃÅ–Ω—Å—å–∫–∞\", \"Ti·∫øng Vi·ªát\", \"ÁÆÄ‰Ωì‰∏≠Êñá\"]\n",
        "#@markdown ---\n",
        "# language definition:\n",
        "languages = {\n",
        "    \"ÿ£ŸÑÿπŸéÿ±Ÿéÿ®ŸêŸä\": \"ar\",\n",
        "    \"Catal√†\": \"ca\",\n",
        "    \"ƒçe≈°tina\": \"cs\",\n",
        "    \"Dansk\": \"da\",\n",
        "    \"Deutsch\": \"de\",\n",
        "    \"ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨\": \"el\",\n",
        "    \"English (British)\": \"en\",\n",
        "    \"English (U.S.)\": \"en-us\",\n",
        "    \"Espa√±ol (Castellano)\": \"es\",\n",
        "    \"Espa√±ol (Latinoamericano)\": \"es-419\",\n",
        "    \"Suomi.\": \"fi\",\n",
        "    \"Fran√ßais\": \"fr\",\n",
        "    \"Magyar\": \"hu\",\n",
        "    \"Icelandic\": \"is\",\n",
        "    \"Italiano\": \"it\",\n",
        "    \"·É•·Éê·É†·Éó·É£·Éö·Éò\": \"ka\",\n",
        "    \"“õ–∞–∑–∞“õ—à–∞\": \"kk\",\n",
        "    \"L√´tzebuergesch\": \"lb\",\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤‡•Ä\": \"ne\",\n",
        "    \"Nederlands\": \"nl\",\n",
        "    \"Norsk\": \"nb\",\n",
        "    \"Polski\": \"pl\",\n",
        "    \"Portugu√™s (Brasil)\": \"pt-br\",\n",
        "    \"Portugu√™s (Portugal)\": \"pt-pt\",\n",
        "    \"Rom√¢nƒÉ\": \"ro\",\n",
        "    \"–†—É—Å—Å–∫–∏–π\": \"ru\",\n",
        "    \"–°—Ä–ø—Å–∫–∏\": \"sr\",\n",
        "    \"Svenska\": \"sv\",\n",
        "    \"Kiswahili\": \"sw\",\n",
        "    \"T√ºrk√ße\": \"tr\",\n",
        "    \"—É–∫—Ä–∞—óÃÅ–Ω—Å—å–∫–∞\": \"uk\",\n",
        "    \"Ti·∫øng Vi·ªát\": \"vi\",\n",
        "    \"ÁÆÄ‰Ωì‰∏≠Êñá\": \"zh\"\n",
        "}\n",
        "\n",
        "def _get_language(code):\n",
        "    return languages[code]\n",
        "\n",
        "final_language = _get_language(language)\n",
        "#@markdown ### Choose a name for your model:\n",
        "model_name = \"kaori\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "# output:\n",
        "#@markdown ### Choose the working folder: (recommended to save to Drive)\n",
        "\n",
        "#@markdown The working folder will be used in preprocessing, but also in training the model.\n",
        "output_path = \"/content/kaori\" #@param {type:\"string\"}\n",
        "output_dir = output_path+\"/\"+model_name\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "#@markdown ---\n",
        "#@markdown ### Choose dataset format:\n",
        "dataset_format = \"ljspeech\" #@param [\"ljspeech\", \"mycroft\"]\n",
        "#@markdown ---\n",
        "#@markdown ### Is this a single speaker dataset? Otherwise, uncheck:\n",
        "single_speaker = True #@param {type:\"boolean\"}\n",
        "if single_speaker:\n",
        "  force_sp = \" --single-speaker\"\n",
        "else:\n",
        "  force_sp = \"\"\n",
        "#@markdown ---\n",
        "#@markdown ### Select the sample rate of the dataset:\n",
        "sample_rate = \"22050\" #@param [\"16000\", \"22050\"]\n",
        "#@markdown ---\n",
        "# creating paths:\n",
        "if not os.path.exists(\"/content/audio_cache\"):\n",
        "    os.makedirs(\"/content/audio_cache\")\n",
        "%cd /content/piper/src/python\n",
        "#@markdown ### Do you want to train using this sample rate, but your audios don't have it?\n",
        "#@markdown The resampler helps you do it quickly!\n",
        "resample = False #@param {type:\"boolean\"}\n",
        "if resample:\n",
        "  !python resample.py --input_dir \"/content/dataset/wavs\" --output_dir \"/content/dataset/wavs_resampled\" --output_sr {sample_rate} --file_ext \"wav\"\n",
        "  !mv /content/dataset/wavs_resampled/* /content/dataset/wavs\n",
        "#@markdown ---\n",
        "# check transcription:\n",
        "if use_whisper:\n",
        "    print(\"Transcript file hasn't been uploaded. Transcribing these audios using Whisper...\")\n",
        "    make_dataset(\"/content/dataset/wavs\", final_language[:2])\n",
        "    print(\"Transcription done! Pre-processing...\")\n",
        "!python -m piper_train.preprocess \\\n",
        "  --language {final_language} \\\n",
        "  --input-dir /content/dataset \\\n",
        "  --cache-dir \"/content/audio_cache\" \\\n",
        "  --output-dir \"{output_dir}\" \\\n",
        "  --dataset-name \"{model_name}\" \\\n",
        "  --dataset-format {dataset_format} \\\n",
        "  --sample-rate {sample_rate} \\\n",
        "  {force_sp}\n",
        "print(\"Preprocessing done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cellView": "form",
        "id": "ickQlOCRjkBL",
        "outputId": "74828589-617b-48ca-da80-87096a004e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "73498997d0e74aa1bf13af22f23b046f",
            "0358c23e72c34ed9a8bfb0c6e519056a",
            "a1f7b2e0f69840f7bb049621d5ca7cd4",
            "860f2348a7884f5bb60caf801331b43e",
            "0edf7f7998d548da8a39ef4ed515ee10",
            "a15e6929ed364bcba6a518c085c46398"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mModel downloaded!\n"
          ]
        }
      ],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **4. Settings.** üß∞\n",
        "#@markdown ---\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from google.colab import output\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "#@markdown ### <font color=\"orange\">**Select the action to train this dataset: (READ CAREFULLY)**\n",
        "\n",
        "#@markdown * The option to <font color=\"orange\">continue a training</font> is self-explanatory. If you've previously trained a model with free colab, your time is up and you're considering training it some more, this is ideal for you. You just have to set the same settings that you set when you first trained this model.\n",
        "#@markdown * The option to <font color=\"orange\">convert a single-speaker model to a multi-speaker model</font> is self-explanatory, and for this it is important that you have processed a dataset that contains text and audio from all possible speakers that you want to train in your model.\n",
        "#@markdown * The <font color=\"orange\">finetune</font> option is used to train a dataset using a pretrained model, that is, train on that data. This option is ideal if you want to train a very small dataset (more than five minutes recommended).\n",
        "#@markdown * The <font color=\"orange\">train from scratch</font> option builds features such as dictionary and speech form from scratch, and this may take longer to converge. For this, hours of audio (8 at least) are recommended, which have a large collection of phonemes.\n",
        "\n",
        "action = \"finetune\" #@param [\"Continue training\", \"convert single-speaker to multi-speaker model\", \"finetune\", \"train from scratch\"]\n",
        "#@markdown ---\n",
        "if action == \"Continue training\":\n",
        "    checkpoints = glob.glob(f\"{output_dir}/lightning_logs/**/checkpoints/last.ckpt\", recursive=True)\n",
        "    if len(checkpoints):\n",
        "        last_checkpoint = sorted(checkpoints, key=lambda x: int(re.findall(r'version_(\\d+)', x)[0]))[-1]\n",
        "        ft_command = f'--resume_from_checkpoint \"{last_checkpoint}\" '\n",
        "        print(f\"Continuing {model_name}'s training at: {last_checkpoint}\")\n",
        "    else:\n",
        "        raise Exception(\"Training cannot be continued as there is no checkpoint to continue at.\")\n",
        "elif action == \"finetune\":\n",
        "    if os.path.exists(f\"{output_dir}/lightning_logs/version_0/checkpoints/last.ckpt\"):\n",
        "        raise Exception(\"Oh no! You have already trained this model before, you cannot choose this option since your progress will be lost, and then your previous time will not count. Please select the option to continue a training.\")\n",
        "    else:\n",
        "        ft_command = '--resume_from_checkpoint \"/content/pretrained.ckpt\" '\n",
        "elif action == \"convert single-speaker to multi-speaker model\":\n",
        "    if not single_speaker:\n",
        "        ft_command = '--resume_from_single_speaker_checkpoint \"/content/pretrained.ckpt\" '\n",
        "    else:\n",
        "        raise Exception(\"This dataset is not a multi-speaker dataset!\")\n",
        "else:\n",
        "    ft_command = \"\"\n",
        "if action== \"convert single-speaker to multi-speaker model\" or action == \"finetune\":\n",
        "    try:\n",
        "        with open('/content/piper/notebooks/pretrained_models.json') as f:\n",
        "            pretrained_models = json.load(f)\n",
        "        if final_language in pretrained_models:\n",
        "            models = pretrained_models[final_language]\n",
        "            model_options = [(model_name, model_name) for model_name, model_url in models.items()]\n",
        "            model_dropdown = widgets.Dropdown(description = \"Choose pretrained model\", options=model_options)\n",
        "            download_button = widgets.Button(description=\"Download\")\n",
        "            def download_model(btn):\n",
        "                model_name = model_dropdown.value\n",
        "                model_url = pretrained_models[final_language][model_name]\n",
        "                print(\"\\033[93mDownloading pretrained model...\")\n",
        "                if model_url.startswith(\"1\"):\n",
        "                    !gdown -q \"{model_url}\" -O \"/content/pretrained.ckpt\"\n",
        "                elif model_url.startswith(\"https://drive.google.com/file/d/\"):\n",
        "                    !gdown -q \"{model_url}\" -O \"/content/pretrained.ckpt\" --fuzzy\n",
        "                else:\n",
        "                    !wget -q \"{model_url}\" -O \"/content/pretrained.ckpt\"\n",
        "                model_dropdown.close()\n",
        "                download_button.close()\n",
        "                output.clear()\n",
        "                if os.path.exists(\"/content/pretrained.ckpt\"):\n",
        "                    print(\"\\033[93mModel downloaded!\")\n",
        "                else:\n",
        "                    raise Exception(\"Couldn't download the pretrained model!\")\n",
        "            download_button.on_click(download_model)\n",
        "            display(model_dropdown, download_button)\n",
        "        else:\n",
        "            raise Exception(f\"There are no pretrained models available for the language {final_language}\")\n",
        "    except FileNotFoundError:\n",
        "        raise Exception(\"The pretrained_models.json file was not found.\")\n",
        "else:\n",
        "    print(\"\\033[93mWarning: this model will be trained from scratch. You need at least 8 hours of data for everything to work decent. Good luck!\")\n",
        "#@markdown ### Choose batch size based on this dataset:\n",
        "batch_size = 12 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ### Choose the quality for this model:\n",
        "\n",
        "#@markdown * x-low - 16Khz audio, 5-7M params\n",
        "#@markdown * medium - 22.05Khz audio, 15-20 params\n",
        "#@markdown * high - 22.05Khz audio, 28-32M params\n",
        "quality = \"high\" #@param [\"high\", \"x-low\", \"medium\"]\n",
        "#@markdown ---\n",
        "#@markdown ### For how many epochs to save training checkpoints?\n",
        "#@markdown The larger your dataset, you should set this saving interval to a smaller value, as epochs can progress longer time.\n",
        "checkpoint_epochs = 5 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "#@markdown ### Interval to save best k models:\n",
        "#@markdown Set to 0 if you want to disable saving multiple models. If this is the case, check the checkbox below. If set to 1, models will be saved with the file name epoch=xx-step=xx.ckpt, so you will need to empty Drive's trash every so often.\n",
        "num_ckpt = 1 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "#@markdown ### Save latest model:\n",
        "#@markdown This checkbox must be checked if you want to save a single model (last.ckpt). Saving a single model is applied only if num_ckpt is equal to 0. If so, the interval parameter of epochs to save is ignored, since the last model per epoch is saved; also, you won't have to worry about storage. Being equal to 1, last.ckpt will be saved, but another model (model_vVersion.ckpt, the latter takes into account the epoch range you set), so you would have to empty the trash often.\n",
        "\n",
        "#@markdown **It's not recommended to use this option in extremely small datasets, since by saving the last model each epoch, this process will be very fast and the trainer will not be able to save the complete model, which would result in a corrupt last.ckpt.**\n",
        "save_last = False # @param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown ### Step interval to generate model samples:\n",
        "log_every_n_steps = 1000 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "#@markdown ### Training epochs:\n",
        "max_epochs = 10000 #@param {type:\"integer\"}\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MpKDfhAHjHJ3"
      },
      "outputs": [],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **5. Run the TensorBoard extension.** üìà\n",
        "#@markdown ---\n",
        "#@markdown The TensorBoard is used to visualize the results of the model while it's being trained such as audio and losses.\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {output_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4zbSjXg2J3N",
        "outputId": "db75844b-888d-49b7-85eb-286f91d2c5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG:piper_train:Namespace(dataset_dir='/content/kaori/kaori', checkpoint_epochs=5, quality='high', resume_from_single_speaker_checkpoint=None, batch_size=12, validation_split=0.01, num_test_examples=1, max_phoneme_ids=None, hidden_channels=192, inter_channels=192, filter_channels=768, n_layers=6, n_heads=2, lr_decay=0.999875, lr_reduce_enabled=False, lr_reduce_factor=0.5, lr_reduce_patience=10, show_plot=False, plot_save_path=None, learning_rate=0.0002, weight_decay=0.01, override_learning_rate=False, grad_clip=None, accelerator='gpu', devices=1, log_every_n_steps=1000, max_epochs=10000, seed=1234, random_seed=False, resume_from_checkpoint='/content/pretrained.ckpt', precision='32', num_ckpt=1, default_root_dir=None, save_last=None, monitor='val_loss', monitor_mode='min', early_stop_patience=0)\n",
            "DEBUG:piper_train:Using manual seed: 1234\n",
            "DEBUG:piper_train:Checkpoints will be saved every 5 epoch(s)\n",
            "DEBUG:piper_train:1 Checkpoints will be saved\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "DEBUG:vits.dataset:Loading dataset: /content/kaori/kaori/dataset.jsonl\n",
            "2025-04-25 23:35:34.082218: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745624134.102358   16966 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745624134.108481   16966 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-25 23:35:34.128703: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:2025-04-25 23:35:35,738:jax._src.path:31: etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "Restoring states from the checkpoint path at /content/pretrained.ckpt\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.7.7 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../pretrained.ckpt`\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/piper/src/python/piper_train/__main__.py\", line 278, in <module>\n",
            "    main()\n",
            "  File \"/content/piper/src/python/piper_train/__main__.py\", line 255, in main\n",
            "    trainer.fit(model, ckpt_path=args.resume_from_checkpoint)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
            "    return trainer_fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
            "    self._checkpoint_connector._restore_modules_and_callbacks(ckpt_path)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 409, in _restore_modules_and_callbacks\n",
            "    self.restore_model()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 286, in restore_model\n",
            "    self.trainer.strategy.load_model_state_dict(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 372, in load_model_state_dict\n",
            "    self.lightning_module.load_state_dict(checkpoint[\"state_dict\"], strict=strict)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 2581, in load_state_dict\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Error(s) in loading state_dict for VitsModel:\n",
            "\tMissing key(s) in state_dict: \"model_g.dec.ups.3.bias\", \"model_g.dec.ups.3.weight_g\", \"model_g.dec.ups.3.weight_v\", \"model_g.dec.resblocks.0.convs1.0.bias\", \"model_g.dec.resblocks.0.convs1.0.weight_g\", \"model_g.dec.resblocks.0.convs1.0.weight_v\", \"model_g.dec.resblocks.0.convs1.1.bias\", \"model_g.dec.resblocks.0.convs1.1.weight_g\", \"model_g.dec.resblocks.0.convs1.1.weight_v\", \"model_g.dec.resblocks.0.convs1.2.bias\", \"model_g.dec.resblocks.0.convs1.2.weight_g\", \"model_g.dec.resblocks.0.convs1.2.weight_v\", \"model_g.dec.resblocks.0.convs2.0.bias\", \"model_g.dec.resblocks.0.convs2.0.weight_g\", \"model_g.dec.resblocks.0.convs2.0.weight_v\", \"model_g.dec.resblocks.0.convs2.1.bias\", \"model_g.dec.resblocks.0.convs2.1.weight_g\", \"model_g.dec.resblocks.0.convs2.1.weight_v\", \"model_g.dec.resblocks.0.convs2.2.bias\", \"model_g.dec.resblocks.0.convs2.2.weight_g\", \"model_g.dec.resblocks.0.convs2.2.weight_v\", \"model_g.dec.resblocks.1.convs1.0.bias\", \"model_g.dec.resblocks.1.convs1.0.weight_g\", \"model_g.dec.resblocks.1.convs1.0.weight_v\", \"model_g.dec.resblocks.1.convs1.1.bias\", \"model_g.dec.resblocks.1.convs1.1.weight_g\", \"model_g.dec.resblocks.1.convs1.1.weight_v\", \"model_g.dec.resblocks.1.convs1.2.bias\", \"model_g.dec.resblocks.1.convs1.2.weight_g\", \"model_g.dec.resblocks.1.convs1.2.weight_v\", \"model_g.dec.resblocks.1.convs2.0.bias\", \"model_g.dec.resblocks.1.convs2.0.weight_g\", \"model_g.dec.resblocks.1.convs2.0.weight_v\", \"model_g.dec.resblocks.1.convs2.1.bias\", \"model_g.dec.resblocks.1.convs2.1.weight_g\", \"model_g.dec.resblocks.1.convs2.1.weight_v\", \"model_g.dec.resblocks.1.convs2.2.bias\", \"model_g.dec.resblocks.1.convs2.2.weight_g\", \"model_g.dec.resblocks.1.convs2.2.weight_v\", \"model_g.dec.resblocks.2.convs1.0.bias\", \"model_g.dec.resblocks.2.convs1.0.weight_g\", \"model_g.dec.resblocks.2.convs1.0.weight_v\", \"model_g.dec.resblocks.2.convs1.1.bias\", \"model_g.dec.resblocks.2.convs1.1.weight_g\", \"model_g.dec.resblocks.2.convs1.1.weight_v\", \"model_g.dec.resblocks.2.convs1.2.bias\", \"model_g.dec.resblocks.2.convs1.2.weight_g\", \"model_g.dec.resblocks.2.convs1.2.weight_v\", \"model_g.dec.resblocks.2.convs2.0.bias\", \"model_g.dec.resblocks.2.convs2.0.weight_g\", \"model_g.dec.resblocks.2.convs2.0.weight_v\", \"model_g.dec.resblocks.2.convs2.1.bias\", \"model_g.dec.resblocks.2.convs2.1.weight_g\", \"model_g.dec.resblocks.2.convs2.1.weight_v\", \"model_g.dec.resblocks.2.convs2.2.bias\", \"model_g.dec.resblocks.2.convs2.2.weight_g\", \"model_g.dec.resblocks.2.convs2.2.weight_v\", \"model_g.dec.resblocks.3.convs1.0.bias\", \"model_g.dec.resblocks.3.convs1.0.weight_g\", \"model_g.dec.resblocks.3.convs1.0.weight_v\", \"model_g.dec.resblocks.3.convs1.1.bias\", \"model_g.dec.resblocks.3.convs1.1.weight_g\", \"model_g.dec.resblocks.3.convs1.1.weight_v\", \"model_g.dec.resblocks.3.convs1.2.bias\", \"model_g.dec.resblocks.3.convs1.2.weight_g\", \"model_g.dec.resblocks.3.convs1.2.weight_v\", \"model_g.dec.resblocks.3.convs2.0.bias\", \"model_g.dec.resblocks.3.convs2.0.weight_g\", \"model_g.dec.resblocks.3.convs2.0.weight_v\", \"model_g.dec.resblocks.3.convs2.1.bias\", \"model_g.dec.resblocks.3.convs2.1.weight_g\", \"model_g.dec.resblocks.3.convs2.1.weight_v\", \"model_g.dec.resblocks.3.convs2.2.bias\", \"model_g.dec.resblocks.3.convs2.2.weight_g\", \"model_g.dec.resblocks.3.convs2.2.weight_v\", \"model_g.dec.resblocks.4.convs1.0.bias\", \"model_g.dec.resblocks.4.convs1.0.weight_g\", \"model_g.dec.resblocks.4.convs1.0.weight_v\", \"model_g.dec.resblocks.4.convs1.1.bias\", \"model_g.dec.resblocks.4.convs1.1.weight_g\", \"model_g.dec.resblocks.4.convs1.1.weight_v\", \"model_g.dec.resblocks.4.convs1.2.bias\", \"model_g.dec.resblocks.4.convs1.2.weight_g\", \"model_g.dec.resblocks.4.convs1.2.weight_v\", \"model_g.dec.resblocks.4.convs2.0.bias\", \"model_g.dec.resblocks.4.convs2.0.weight_g\", \"model_g.dec.resblocks.4.convs2.0.weight_v\", \"model_g.dec.resblocks.4.convs2.1.bias\", \"model_g.dec.resblocks.4.convs2.1.weight_g\", \"model_g.dec.resblocks.4.convs2.1.weight_v\", \"model_g.dec.resblocks.4.convs2.2.bias\", \"model_g.dec.resblocks.4.convs2.2.weight_g\", \"model_g.dec.resblocks.4.convs2.2.weight_v\", \"model_g.dec.resblocks.5.convs1.0.bias\", \"model_g.dec.resblocks.5.convs1.0.weight_g\", \"model_g.dec.resblocks.5.convs1.0.weight_v\", \"model_g.dec.resblocks.5.convs1.1.bias\", \"model_g.dec.resblocks.5.convs1.1.weight_g\", \"model_g.dec.resblocks.5.convs1.1.weight_v\", \"model_g.dec.resblocks.5.convs1.2.bias\", \"model_g.dec.resblocks.5.convs1.2.weight_g\", \"model_g.dec.resblocks.5.convs1.2.weight_v\", \"model_g.dec.resblocks.5.convs2.0.bias\", \"model_g.dec.resblocks.5.convs2.0.weight_g\", \"model_g.dec.resblocks.5.convs2.0.weight_v\", \"model_g.dec.resblocks.5.convs2.1.bias\", \"model_g.dec.resblocks.5.convs2.1.weight_g\", \"model_g.dec.resblocks.5.convs2.1.weight_v\", \"model_g.dec.resblocks.5.convs2.2.bias\", \"model_g.dec.resblocks.5.convs2.2.weight_g\", \"model_g.dec.resblocks.5.convs2.2.weight_v\", \"model_g.dec.resblocks.6.convs1.0.bias\", \"model_g.dec.resblocks.6.convs1.0.weight_g\", \"model_g.dec.resblocks.6.convs1.0.weight_v\", \"model_g.dec.resblocks.6.convs1.1.bias\", \"model_g.dec.resblocks.6.convs1.1.weight_g\", \"model_g.dec.resblocks.6.convs1.1.weight_v\", \"model_g.dec.resblocks.6.convs1.2.bias\", \"model_g.dec.resblocks.6.convs1.2.weight_g\", \"model_g.dec.resblocks.6.convs1.2.weight_v\", \"model_g.dec.resblocks.6.convs2.0.bias\", \"model_g.dec.resblocks.6.convs2.0.weight_g\", \"model_g.dec.resblocks.6.convs2.0.weight_v\", \"model_g.dec.resblocks.6.convs2.1.bias\", \"model_g.dec.resblocks.6.convs2.1.weight_g\", \"model_g.dec.resblocks.6.convs2.1.weight_v\", \"model_g.dec.resblocks.6.convs2.2.bias\", \"model_g.dec.resblocks.6.convs2.2.weight_g\", \"model_g.dec.resblocks.6.convs2.2.weight_v\", \"model_g.dec.resblocks.7.convs1.0.bias\", \"model_g.dec.resblocks.7.convs1.0.weight_g\", \"model_g.dec.resblocks.7.convs1.0.weight_v\", \"model_g.dec.resblocks.7.convs1.1.bias\", \"model_g.dec.resblocks.7.convs1.1.weight_g\", \"model_g.dec.resblocks.7.convs1.1.weight_v\", \"model_g.dec.resblocks.7.convs1.2.bias\", \"model_g.dec.resblocks.7.convs1.2.weight_g\", \"model_g.dec.resblocks.7.convs1.2.weight_v\", \"model_g.dec.resblocks.7.convs2.0.bias\", \"model_g.dec.resblocks.7.convs2.0.weight_g\", \"model_g.dec.resblocks.7.convs2.0.weight_v\", \"model_g.dec.resblocks.7.convs2.1.bias\", \"model_g.dec.resblocks.7.convs2.1.weight_g\", \"model_g.dec.resblocks.7.convs2.1.weight_v\", \"model_g.dec.resblocks.7.convs2.2.bias\", \"model_g.dec.resblocks.7.convs2.2.weight_g\", \"model_g.dec.resblocks.7.convs2.2.weight_v\", \"model_g.dec.resblocks.8.convs1.0.bias\", \"model_g.dec.resblocks.8.convs1.0.weight_g\", \"model_g.dec.resblocks.8.convs1.0.weight_v\", \"model_g.dec.resblocks.8.convs1.1.bias\", \"model_g.dec.resblocks.8.convs1.1.weight_g\", \"model_g.dec.resblocks.8.convs1.1.weight_v\", \"model_g.dec.resblocks.8.convs1.2.bias\", \"model_g.dec.resblocks.8.convs1.2.weight_g\", \"model_g.dec.resblocks.8.convs1.2.weight_v\", \"model_g.dec.resblocks.8.convs2.0.bias\", \"model_g.dec.resblocks.8.convs2.0.weight_g\", \"model_g.dec.resblocks.8.convs2.0.weight_v\", \"model_g.dec.resblocks.8.convs2.1.bias\", \"model_g.dec.resblocks.8.convs2.1.weight_g\", \"model_g.dec.resblocks.8.convs2.1.weight_v\", \"model_g.dec.resblocks.8.convs2.2.bias\", \"model_g.dec.resblocks.8.convs2.2.weight_g\", \"model_g.dec.resblocks.8.convs2.2.weight_v\", \"model_g.dec.resblocks.9.convs1.0.bias\", \"model_g.dec.resblocks.9.convs1.0.weight_g\", \"model_g.dec.resblocks.9.convs1.0.weight_v\", \"model_g.dec.resblocks.9.convs1.1.bias\", \"model_g.dec.resblocks.9.convs1.1.weight_g\", \"model_g.dec.resblocks.9.convs1.1.weight_v\", \"model_g.dec.resblocks.9.convs1.2.bias\", \"model_g.dec.resblocks.9.convs1.2.weight_g\", \"model_g.dec.resblocks.9.convs1.2.weight_v\", \"model_g.dec.resblocks.9.convs2.0.bias\", \"model_g.dec.resblocks.9.convs2.0.weight_g\", \"model_g.dec.resblocks.9.convs2.0.weight_v\", \"model_g.dec.resblocks.9.convs2.1.bias\", \"model_g.dec.resblocks.9.convs2.1.weight_g\", \"model_g.dec.resblocks.9.convs2.1.weight_v\", \"model_g.dec.resblocks.9.convs2.2.bias\", \"model_g.dec.resblocks.9.convs2.2.weight_g\", \"model_g.dec.resblocks.9.convs2.2.weight_v\", \"model_g.dec.resblocks.10.convs1.0.bias\", \"model_g.dec.resblocks.10.convs1.0.weight_g\", \"model_g.dec.resblocks.10.convs1.0.weight_v\", \"model_g.dec.resblocks.10.convs1.1.bias\", \"model_g.dec.resblocks.10.convs1.1.weight_g\", \"model_g.dec.resblocks.10.convs1.1.weight_v\", \"model_g.dec.resblocks.10.convs1.2.bias\", \"model_g.dec.resblocks.10.convs1.2.weight_g\", \"model_g.dec.resblocks.10.convs1.2.weight_v\", \"model_g.dec.resblocks.10.convs2.0.bias\", \"model_g.dec.resblocks.10.convs2.0.weight_g\", \"model_g.dec.resblocks.10.convs2.0.weight_v\", \"model_g.dec.resblocks.10.convs2.1.bias\", \"model_g.dec.resblocks.10.convs2.1.weight_g\", \"model_g.dec.resblocks.10.convs2.1.weight_v\", \"model_g.dec.resblocks.10.convs2.2.bias\", \"model_g.dec.resblocks.10.convs2.2.weight_g\", \"model_g.dec.resblocks.10.convs2.2.weight_v\", \"model_g.dec.resblocks.11.convs1.0.bias\", \"model_g.dec.resblocks.11.convs1.0.weight_g\", \"model_g.dec.resblocks.11.convs1.0.weight_v\", \"model_g.dec.resblocks.11.convs1.1.bias\", \"model_g.dec.resblocks.11.convs1.1.weight_g\", \"model_g.dec.resblocks.11.convs1.1.weight_v\", \"model_g.dec.resblocks.11.convs1.2.bias\", \"model_g.dec.resblocks.11.convs1.2.weight_g\", \"model_g.dec.resblocks.11.convs1.2.weight_v\", \"model_g.dec.resblocks.11.convs2.0.bias\", \"model_g.dec.resblocks.11.convs2.0.weight_g\", \"model_g.dec.resblocks.11.convs2.0.weight_v\", \"model_g.dec.resblocks.11.convs2.1.bias\", \"model_g.dec.resblocks.11.convs2.1.weight_g\", \"model_g.dec.resblocks.11.convs2.1.weight_v\", \"model_g.dec.resblocks.11.convs2.2.bias\", \"model_g.dec.resblocks.11.convs2.2.weight_g\", \"model_g.dec.resblocks.11.convs2.2.weight_v\". \n",
            "\tUnexpected key(s) in state_dict: \"model_g.emb_g.weight\", \"model_g.dec.cond.weight\", \"model_g.dec.cond.bias\", \"model_g.dec.resblocks.0.convs.0.bias\", \"model_g.dec.resblocks.0.convs.0.weight_g\", \"model_g.dec.resblocks.0.convs.0.weight_v\", \"model_g.dec.resblocks.0.convs.1.bias\", \"model_g.dec.resblocks.0.convs.1.weight_g\", \"model_g.dec.resblocks.0.convs.1.weight_v\", \"model_g.dec.resblocks.1.convs.0.bias\", \"model_g.dec.resblocks.1.convs.0.weight_g\", \"model_g.dec.resblocks.1.convs.0.weight_v\", \"model_g.dec.resblocks.1.convs.1.bias\", \"model_g.dec.resblocks.1.convs.1.weight_g\", \"model_g.dec.resblocks.1.convs.1.weight_v\", \"model_g.dec.resblocks.2.convs.0.bias\", \"model_g.dec.resblocks.2.convs.0.weight_g\", \"model_g.dec.resblocks.2.convs.0.weight_v\", \"model_g.dec.resblocks.2.convs.1.bias\", \"model_g.dec.resblocks.2.convs.1.weight_g\", \"model_g.dec.resblocks.2.convs.1.weight_v\", \"model_g.dec.resblocks.3.convs.0.bias\", \"model_g.dec.resblocks.3.convs.0.weight_g\", \"model_g.dec.resblocks.3.convs.0.weight_v\", \"model_g.dec.resblocks.3.convs.1.bias\", \"model_g.dec.resblocks.3.convs.1.weight_g\", \"model_g.dec.resblocks.3.convs.1.weight_v\", \"model_g.dec.resblocks.4.convs.0.bias\", \"model_g.dec.resblocks.4.convs.0.weight_g\", \"model_g.dec.resblocks.4.convs.0.weight_v\", \"model_g.dec.resblocks.4.convs.1.bias\", \"model_g.dec.resblocks.4.convs.1.weight_g\", \"model_g.dec.resblocks.4.convs.1.weight_v\", \"model_g.dec.resblocks.5.convs.0.bias\", \"model_g.dec.resblocks.5.convs.0.weight_g\", \"model_g.dec.resblocks.5.convs.0.weight_v\", \"model_g.dec.resblocks.5.convs.1.bias\", \"model_g.dec.resblocks.5.convs.1.weight_g\", \"model_g.dec.resblocks.5.convs.1.weight_v\", \"model_g.dec.resblocks.6.convs.0.bias\", \"model_g.dec.resblocks.6.convs.0.weight_g\", \"model_g.dec.resblocks.6.convs.0.weight_v\", \"model_g.dec.resblocks.6.convs.1.bias\", \"model_g.dec.resblocks.6.convs.1.weight_g\", \"model_g.dec.resblocks.6.convs.1.weight_v\", \"model_g.dec.resblocks.7.convs.0.bias\", \"model_g.dec.resblocks.7.convs.0.weight_g\", \"model_g.dec.resblocks.7.convs.0.weight_v\", \"model_g.dec.resblocks.7.convs.1.bias\", \"model_g.dec.resblocks.7.convs.1.weight_g\", \"model_g.dec.resblocks.7.convs.1.weight_v\", \"model_g.dec.resblocks.8.convs.0.bias\", \"model_g.dec.resblocks.8.convs.0.weight_g\", \"model_g.dec.resblocks.8.convs.0.weight_v\", \"model_g.dec.resblocks.8.convs.1.bias\", \"model_g.dec.resblocks.8.convs.1.weight_g\", \"model_g.dec.resblocks.8.convs.1.weight_v\", \"model_g.enc_q.enc.cond_layer.bias\", \"model_g.enc_q.enc.cond_layer.weight_g\", \"model_g.enc_q.enc.cond_layer.weight_v\", \"model_g.flow.flows.0.enc.cond_layer.bias\", \"model_g.flow.flows.0.enc.cond_layer.weight_g\", \"model_g.flow.flows.0.enc.cond_layer.weight_v\", \"model_g.flow.flows.2.enc.cond_layer.bias\", \"model_g.flow.flows.2.enc.cond_layer.weight_g\", \"model_g.flow.flows.2.enc.cond_layer.weight_v\", \"model_g.flow.flows.4.enc.cond_layer.bias\", \"model_g.flow.flows.4.enc.cond_layer.weight_g\", \"model_g.flow.flows.4.enc.cond_layer.weight_v\", \"model_g.flow.flows.6.enc.cond_layer.bias\", \"model_g.flow.flows.6.enc.cond_layer.weight_g\", \"model_g.flow.flows.6.enc.cond_layer.weight_v\", \"model_g.dp.cond.weight\", \"model_g.dp.cond.bias\". \n",
            "\tsize mismatch for model_g.dec.conv_pre.weight: copying a param with shape torch.Size([256, 192, 7]) from checkpoint, the shape in current model is torch.Size([512, 192, 7]).\n",
            "\tsize mismatch for model_g.dec.conv_pre.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for model_g.dec.ups.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n",
            "\tsize mismatch for model_g.dec.ups.0.weight_g: copying a param with shape torch.Size([256, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1, 1]).\n",
            "\tsize mismatch for model_g.dec.ups.0.weight_v: copying a param with shape torch.Size([256, 128, 16]) from checkpoint, the shape in current model is torch.Size([512, 256, 16]).\n",
            "\tsize mismatch for model_g.dec.ups.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n",
            "\tsize mismatch for model_g.dec.ups.1.weight_g: copying a param with shape torch.Size([128, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 1, 1]).\n",
            "\tsize mismatch for model_g.dec.ups.1.weight_v: copying a param with shape torch.Size([128, 64, 16]) from checkpoint, the shape in current model is torch.Size([256, 128, 16]).\n",
            "\tsize mismatch for model_g.dec.ups.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n",
            "\tsize mismatch for model_g.dec.ups.2.weight_g: copying a param with shape torch.Size([64, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 1, 1]).\n",
            "\tsize mismatch for model_g.dec.ups.2.weight_v: copying a param with shape torch.Size([64, 32, 8]) from checkpoint, the shape in current model is torch.Size([128, 64, 4]).\n"
          ]
        }
      ],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **6. Train.** üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
        "#@markdown ---\n",
        "#@markdown ### Run this cell to train your final model!\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### <font color=\"orange\">**Disable validation?**\n",
        "#@markdown By uncheck this checkbox, this will allow to train the full dataset, without using any audio files or examples as a validation set. So, it will not be able to generate audios on the tensorboard while it's training. It is recommended to disable validation on extremely small datasets.\n",
        "validation = True #@param {type:\"boolean\"}\n",
        "if validation:\n",
        "    validation_split = 0.01\n",
        "    num_test_examples = 1\n",
        "else:\n",
        "    validation_split = 0\n",
        "    num_test_examples = 0\n",
        "if not save_last:\n",
        "    save_last_command = \"\"\n",
        "else:\n",
        "    save_last_command = \"--save_last True \"\n",
        "get_ipython().system(f'''\n",
        "python -m piper_train \\\n",
        "--dataset-dir \"{output_dir}\" \\\n",
        "--accelerator 'gpu' \\\n",
        "--devices 1 \\\n",
        "--batch-size {batch_size} \\\n",
        "--validation-split {validation_split} \\\n",
        "--num-test-examples {num_test_examples} \\\n",
        "--quality {quality} \\\n",
        "--checkpoint-epochs {checkpoint_epochs} \\\n",
        "--num_ckpt {num_ckpt} \\\n",
        "{save_last_command}\\\n",
        "--log_every_n_steps {log_every_n_steps} \\\n",
        "--max_epochs {max_epochs} \\\n",
        "{ft_command}\\\n",
        "--precision 32\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ISG085SYn85"
      },
      "source": [
        "#  <font color=\"orange\">**Have you finished training and want to test the model?**\n",
        "\n",
        "* If you want to run this model in any software that Piper integrates or the same Piper app, export your model using the [model exporter notebook](https://colab.research.google.com/github/rmcpantoja/piper/blob/master/notebooks/piper_model_exporter.ipynb)!\n",
        "* Wait! I want to test this right now before exporting it to the supported format for Piper. Test your generated last.ckpt with [this notebook](https://colab.research.google.com/github/rmcpantoja/piper/blob/master/notebooks/piper_inference_(ckpt).ipynb)!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "73498997d0e74aa1bf13af22f23b046f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "davefx-medium (fine-tuned)",
              "sharvard-medium (fine-tuned)"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Choose pretrained model",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_0358c23e72c34ed9a8bfb0c6e519056a",
            "style": "IPY_MODEL_a1f7b2e0f69840f7bb049621d5ca7cd4"
          }
        },
        "0358c23e72c34ed9a8bfb0c6e519056a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1f7b2e0f69840f7bb049621d5ca7cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "860f2348a7884f5bb60caf801331b43e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Download",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_0edf7f7998d548da8a39ef4ed515ee10",
            "style": "IPY_MODEL_a15e6929ed364bcba6a518c085c46398",
            "tooltip": ""
          }
        },
        "0edf7f7998d548da8a39ef4ed515ee10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a15e6929ed364bcba6a518c085c46398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}